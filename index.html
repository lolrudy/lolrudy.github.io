<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ruida Zhang</title>

  <meta name="author" content="Ruida Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Ruida Zhang (Âº†ÁùøËææ)</name>
                  </p>
                  <p>I am a third-year Ph.D. student at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua
                      University</a>,
                    supervised by Prof.
                    <a href="https://www.au.tsinghua.edu.cn/info/1111/1524.htm">Xiangyang Ji</a>.
                    Previously, I received my B.E. degree in Automation Engineering at Tsinghua University.
                    <br>
                    <br>
                    My research interest lies in <b>3D computer vision</b>,
                    including <b>object pose estimation, 3D reconstruction </b> and <b> scene understanding</b>.
                    <br>
                    <br>
                    I served as a reviewer for CVPR, ICCV, ECCV, RAL, TCSVT, TIM.
                    <br>
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:zhangrd23@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=J4u6VicAAAAJ&hl=zh-CN">Google Scholar</a>
                    &nbsp/&nbsp
                    <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                    <a href="https://github.com/lolrudy/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/rudy.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/rudy.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <heading>News</heading>
          <ul>
          <li>[2024.02] 4 papers are accepted to <b>CVPR2024</b>.
          KP-RED and ShapeMaker focus on joint shape canonicalization, segmentation, retrieval and deformation. 
          SecondPose outperforms competitors on category-level pose estimation. 
          MOHO leverages multi-view information for hand-held object reconstruction.
          </li>
          <li>
            [2023.10] Our work GPose2023 <b>wins BOP Challenge 2023</b>, ICCV R6D Workshop. 
          </li>
          <li>
          [2023.09] Our paper DDF-HO on hand-held object reconstruction is accepted to <b>NeurIPS2023</b>.
          </li>
          <li>
          [2023.07] Our paper U-RED on unsupervised shape retrieval and deformation is accepted to <b>ICCV2023</b>.
        </li>
        <li>
          [2022.06] Our category-level pose estimation works GPV-Pose, RBP-Pose, SSP-Pose are accepted to <b>CVPR2022, ECCV2022, IROS2022</b> respectively.
        </li>  
        </ul>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>

                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
            <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/kpred.png' width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>
                  KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation
                  </papertitle>
                  <br>
                  <b>Ruida Zhang</b>*,
                  Chenyangguang Zhang*, Yan Di, Fabian Manhardt, Xingyu Liu, Federico Tombari, Xiangyang Ji
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2403.10099.pdf">Paper</a>
                  <p></p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/shapematcher.png' width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>
                    ShapeMatcher: Self-Supervised Joint Shape Canonicalization, Segmentation, Retrieval and Deformation
                  </papertitle>
                  <br>
                  Yan Di, Chenyangguang Zhang, Chaowei Wang, <b>Ruida Zhang</b>, Guangyao Zhai, Yanyan Li, Bowen Fu, Xiangyang Ji, Shan Gao
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2311.11106">Paper</a>
                  <p></p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/moho.png' width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>
                    MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision
                  </papertitle>
                  <br>
                  Chenyangguang Zhang, Guanlong Jiao, Yan Di, Gu Wang, Ziqin Huang, <b>Ruida Zhang</b>, Fabian Manhardt, Bowen Fu, Federico Tombari, Xiangyang Ji                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2310.11696">Paper</a>
                  <p></p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/gpose.jpg' width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>
                  GPose2023: A Modularized Learning-based Object Pose Estimator
                  </papertitle>
                  <br>
                  <b>Ruida Zhang</b>,
                  Ziqin Huang, Gu Wang, Xingyu Liu,
                  Chenyangguang Zhang,
                  Xiangyang Ji
                  <br>
                  <em>International Conference on Computer Vision Workshop (ICCVW)</em>, 2023
                  <br>
                  <b>Winner of BOP Challenge 2023 @ ICCV R6D Workshop. </b>
                  <br>
                  <a href="https://zhangcyg.github.io/figs/GPose_ICCV23.pdf">Slides</a>
                  <p></p>
                </td>
              </tr>
            <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ddfho.png' width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>
                  DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field
                  </papertitle>
                  <br>
                  <a href="https://zhangcyg.github.io/">Chenyangguang Zhang*</a>,
                  <a href="https://shangbuhuan13.github.io/">Yan Di*</a>,
                  <b>Ruida Zhang*</b>,
                  Guangyao Zhai,
                  <a href="https://campar.in.tum.de/Main/FabianManhardt">Fabian Manhardt</a>,
                  <a href="https://federicotombari.github.io/">Federico Tombari</a>,
                  <a href="https://www.au.tsinghua.edu.cn/info/1111/1524.htm">Xiangyang Ji</a>
                  <br>
                  <em>37th Conference on Neural Information Processing Systems (NeurIPS)</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2308.08231">Paper</a>
                  /
                  <a href="https://github.com/ZhangCYG/DDFHO">Code</a>
                  <p></p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ured.png' width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>U-RED: Unsupervised 3D Shape Retrieval and Deformation for Partial Point Clouds
                  </papertitle>
                  <br>
                  <a href="https://shangbuhuan13.github.io/">Yan Di*</a>,
                  <a href="https://zhangcyg.github.io/">Chenyangguang Zhang*</a>,
                  <b>Ruida Zhang*</b>,
                  <a href="https://campar.in.tum.de/Main/FabianManhardt">Fabian Manhardt</a>,
                  Yongzhi Su, Jason Rambach,
                  <a href="https://www.au.tsinghua.edu.cn/info/1111/1524.htm">Xiangyang Ji</a>,
                  <a href="https://federicotombari.github.io/">Federico Tombari</a>
                  <br>
                  <em>International Conference on Computer Vision (ICCV)</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2308.06383">Paper</a>
                  /
                  <a href="https://github.com/ZhangCYG/U-RED">Code</a>
                  <p></p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/gdrnpp.png' width="220">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>GDRNPP: Extending Geometry-Guided Direct Regression Network in 2022
                  </papertitle>
                  <br>
                  Xingyu Liu,
                  <b>Ruida Zhang</b>,
                  Chenyangguang Zhang, Bowen Fu, Jiwen Tang, Xiquan Liang, Jingyi Tang, Xiaotian Cheng, Yukang
                  Zhang, Gu Wang, Xiangyang Ji
                  <br>
                  <em>European Conference on Computer Vision WorkShop (ECCVW)</em>, 2022
                  <br>
                  <b>Winner of BOP Challenge 2022 @ ECCV R6D Workshop.</b>
                  <br>
                  <a href="https://cmp.felk.cvut.cz/sixd/workshop_2022/slides/bop_challenge_2022_results.pdf">Slides</a>
                  /
                  <a href="https://github.com/shanice-l/gdrnpp_bop2022">Code</a>
                  <p></p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/rbp-pose.png' width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation
                  </papertitle>
                  <br>
                  <b>Ruida Zhang*</b>,
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=HSlGGvwAAAAJ">Yan Di*</a>,
                  Zhiqiang Lou,
                  <a href="https://campar.in.tum.de/Main/FabianManhardt">Fabian Manhardt</a>,
                  <a href="https://federicotombari.github.io/">Federico Tombari</a>,
                  <a href="https://www.au.tsinghua.edu.cn/info/1111/1524.htm">Xiangyang Ji</a>
                  <br>
                  <em>European Conference on Computer Vision (ECCV)</em>, 2022
                  <br>
                  <a href="https://arxiv.org/pdf/2208.00237">Paper</a>
                  /
                  <a href="https://github.com/lolrudy/RBP_Pose">Code</a>
                  <p></p>
                  <!-- <p>
              We utilize shape prior adaptation to enable guide bounding box prediction, .
              </p> -->
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/ssp-pose.png' width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>SSP-Pose: Symmetry-Aware Shape Prior Deformation for Direct Category-Level Object Pose
                    Estimation </papertitle>
                  <br>
                  <b>Ruida Zhang*</b>,
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=HSlGGvwAAAAJ">Yan Di*</a>,
                  <a href="https://campar.in.tum.de/Main/FabianManhardt">Fabian Manhardt</a>,
                  <a href="https://federicotombari.github.io/">Federico Tombari</a>,
                  <a href="https://www.au.tsinghua.edu.cn/info/1111/1524.htm">Xiangyang Ji</a>

                  <br>
                  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2208.06661">Paper</a>
                  <p></p>
                  <!-- <p>
                We integrate category-level shape prior into direct pose estimation to improve performance while maitainng real-time inference speed.
              </p> -->
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/gpv-pose.png' width="160">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting
                  </papertitle>
                  <br>
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=HSlGGvwAAAAJ">Yan Di*</a>,
                  <b>Ruida Zhang*</b>,
                  Zhiqiang Lou,
                  <a href="https://campar.in.tum.de/Main/FabianManhardt">Fabian Manhardt</a>,
                  <a href="https://www.au.tsinghua.edu.cn/info/1111/1524.htm">Xiangyang Ji</a>,
                  <a href="https://campar.in.tum.de/Main/NassirNavabCv">Nassir Navab</a>
                  <a href="https://federicotombari.github.io/">Federico Tombari</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                  <br>
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2022/papers/Di_GPV-Pose_Category-Level_Object_Pose_Estimation_via_Geometry-Guided_Point-Wise_Voting_CVPR_2022_paper.pdf">Paper</a>
                  /
                  <a href="https://github.com/lolrudy/GPV_Pose">Code</a>
                  <p></p>
                  <!-- <p>
              We employ point-wise bounding box voting to improve the performance of category-level object pose estimation.
              </p> -->
                </td>
              </tr>

            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
